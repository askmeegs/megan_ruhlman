{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling of Ruhlman Titles\n",
    "\n",
    "Megan O, 4/25\n",
    "\n",
    "Using this tutorial: https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html\n",
    "\n",
    "(we learned about Latent Dirichlet Allocation in NLP on 4/25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Students</th>\n",
       "      <th>Advisor</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Student_Year</th>\n",
       "      <th>Student_Major</th>\n",
       "      <th>Advisor_Major</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997</td>\n",
       "      <td>\"King Lear\" Through Film: Brook and Kozintsev</td>\n",
       "      <td>Alexandra Parsons</td>\n",
       "      <td>Yu Jin Ko</td>\n",
       "      <td>The final scene of Shakespeare's \"King Lear\" p...</td>\n",
       "      <td>1997</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997</td>\n",
       "      <td>The Grotesque Nature of Paradise in Milton's P...</td>\n",
       "      <td>Elizabeth Reich</td>\n",
       "      <td>Jody Mikalachki</td>\n",
       "      <td>In The Interpretacion of Dreams Freud writes t...</td>\n",
       "      <td>1997</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997</td>\n",
       "      <td>Eliminating Stereotypes, Identifying Confoundi...</td>\n",
       "      <td>Aimee Jabro-Young</td>\n",
       "      <td>Paul Wink</td>\n",
       "      <td>This study compared differences in political b...</td>\n",
       "      <td>1998</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Psychology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997</td>\n",
       "      <td>The Art of Hatred: The Representation of Jewis...</td>\n",
       "      <td>Inna Kantor</td>\n",
       "      <td>Frances Malino</td>\n",
       "      <td>During much of the nineteenth century. Europea...</td>\n",
       "      <td>1997</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Jewish Studies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997</td>\n",
       "      <td>The Problems of, and Possibilities for, Norweg...</td>\n",
       "      <td>Ingrid Moen</td>\n",
       "      <td>Barbara Geller</td>\n",
       "      <td>The goal of my talk is to outline the ways in ...</td>\n",
       "      <td>1997</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Religion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year                                              Title           Students  \\\n",
       "0  1997      \"King Lear\" Through Film: Brook and Kozintsev  Alexandra Parsons   \n",
       "1  1997  The Grotesque Nature of Paradise in Milton's P...    Elizabeth Reich   \n",
       "2  1997  Eliminating Stereotypes, Identifying Confoundi...  Aimee Jabro-Young   \n",
       "3  1997  The Art of Hatred: The Representation of Jewis...        Inna Kantor   \n",
       "4  1997  The Problems of, and Possibilities for, Norweg...        Ingrid Moen   \n",
       "\n",
       "           Advisor                                           Abstract  \\\n",
       "0        Yu Jin Ko  The final scene of Shakespeare's \"King Lear\" p...   \n",
       "1  Jody Mikalachki  In The Interpretacion of Dreams Freud writes t...   \n",
       "2        Paul Wink  This study compared differences in political b...   \n",
       "3   Frances Malino  During much of the nineteenth century. Europea...   \n",
       "4   Barbara Geller  The goal of my talk is to outline the ways in ...   \n",
       "\n",
       "  Student_Year Student_Major   Advisor_Major  \n",
       "0         1997   Unspecified         English  \n",
       "1         1997   Unspecified         English  \n",
       "2         1998   Unspecified      Psychology  \n",
       "3         1997   Unspecified  Jewish Studies  \n",
       "4         1997   Unspecified        Religion  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_csv('aclean.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = df['Title'].tolist()\n",
    "titles = [str(t) for t in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "doc_set = titles\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=50, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.059*model + 0.047*brain + 0.032*reson + 0.030*magnet + 0.021*mous + 0.020*receptor + 0.018*build + 0.016*organ + 0.016*rett + 0.015*use'),\n",
       " (1,\n",
       "  u'0.030*poetri + 0.023*s + 0.022*west + 0.020*centuri + 0.020*hall + 0.017*icon + 0.017*pendleton + 0.017*object + 0.015*bilingu + 0.013*address'),\n",
       " (2,\n",
       "  u'0.036*human + 0.023*social + 0.022*titl + 0.017*help + 0.017*collect + 0.015*find + 0.014*engin + 0.014*antimicrobi + 0.013*right + 0.013*s'),\n",
       " (3,\n",
       "  u'0.065*learn + 0.058*interact + 0.044*teach + 0.025*present + 0.025*program + 0.024*physic + 0.020*signal + 0.017*s + 0.017*lab + 0.014*type'),\n",
       " (4,\n",
       "  u'0.067*develop + 0.029*popul + 0.029*search + 0.028*year + 0.021*liber + 0.019*genet + 0.018*problem + 0.017*role + 0.016*archipelago + 0.014*gal'),\n",
       " (5,\n",
       "  u'0.062*east + 0.053*pendleton + 0.050*panel + 0.029*approach + 0.020*re + 0.019*west + 0.018*treatment + 0.017*global + 0.017*139 + 0.017*new'),\n",
       " (6,\n",
       "  u'0.041*s + 0.040*histori + 0.032*polici + 0.026*china + 0.024*relationship + 0.022*child + 0.018*colon + 0.018*beauti + 0.016*imaginari + 0.014*children'),\n",
       " (7,\n",
       "  u'0.153*wellesley + 0.131*colleg + 0.056*student + 0.040*research + 0.023*s + 0.016*well + 0.013*journey + 0.013*100 + 0.012*self + 0.012*past'),\n",
       " (8,\n",
       "  u'0.058*s + 0.039*live + 0.029*format + 0.025*system + 0.023*citi + 0.021*color + 0.018*nativ + 0.015*facial + 0.015*share + 0.014*peac'),\n",
       " (9,\n",
       "  u'0.041*work + 0.037*gender + 0.033*s + 0.030*race + 0.027*societi + 0.027*sex + 0.026*mother + 0.022*late + 0.021*femal + 0.019*class'),\n",
       " (10,\n",
       "  u'0.069*cultur + 0.030*studi + 0.028*gender + 0.023*case + 0.018*japanes + 0.018*properti + 0.017*progress + 0.016*design + 0.015*construct + 0.014*character'),\n",
       " (11,\n",
       "  u'0.202*women + 0.038*femal + 0.034*percept + 0.026*s + 0.023*stereotyp + 0.020*male + 0.012*effect + 0.012*fashion + 0.011*american + 0.011*model'),\n",
       " (12,\n",
       "  u'0.033*person + 0.030*nanoparticl + 0.020*univers + 0.018*studi + 0.018*s + 0.018*challeng + 0.018*base + 0.016*disord + 0.014*relationship + 0.014*examin'),\n",
       " (13,\n",
       "  u'0.068*state + 0.049*unit + 0.032*home + 0.020*tribolium + 0.018*beetl + 0.017*castaneum + 0.017*flour + 0.017*can + 0.017*15 + 0.015*ion'),\n",
       " (14,\n",
       "  u'0.047*biolog + 0.039*short + 0.036*talk + 0.030*preschool + 0.025*scienc + 0.020*children + 0.018*perform + 0.018*influenc + 0.017*gender + 0.016*chemistri'),\n",
       " (15,\n",
       "  u'0.047*american + 0.041*asian + 0.032*s + 0.030*studi + 0.028*contemporari + 0.023*represent + 0.021*cinema + 0.021*emot + 0.017*italian + 0.016*death'),\n",
       " (16,\n",
       "  u'0.066*role + 0.026*regul + 0.017*english + 0.016*investig + 0.016*franc + 0.015*specif + 0.014*anticanc + 0.012*m + 0.012*acquisit + 0.011*instrument'),\n",
       " (17,\n",
       "  u'0.031*chemistri + 0.029*sexual + 0.028*attitud + 0.027*environment + 0.027*intern + 0.026*languag + 0.021*use + 0.018*mri + 0.018*comparison + 0.018*commun'),\n",
       " (18,\n",
       "  u'0.039*t + 0.036*explor + 0.034*self + 0.032*2 + 0.031*1 + 0.024*probabl + 0.020*don + 0.020*black + 0.019*4 + 0.016*african'),\n",
       " (19,\n",
       "  u'0.032*analysi + 0.029*korea + 0.024*narr + 0.017*north + 0.017*cultur + 0.016*adapt + 0.014*becom + 0.014*ill + 0.014*let + 0.012*polit'),\n",
       " (20,\n",
       "  u'0.035*impact + 0.035*educ + 0.034*s + 0.031*america + 0.024*game + 0.020*latin + 0.017*financi + 0.016*molecular + 0.016*comput + 0.015*studi'),\n",
       " (21,\n",
       "  u'0.066*memori + 0.047*stori + 0.027*futur + 0.025*cancer + 0.023*speci + 0.022*write + 0.020*think + 0.016*stabil + 0.015*asia + 0.015*novel'),\n",
       " (22,\n",
       "  u'0.041*s + 0.034*translat + 0.034*public + 0.025*u + 0.024*engag + 0.020*journal + 0.018*militari + 0.016*across + 0.016*invis + 0.016*plant'),\n",
       " (23,\n",
       "  u'0.052*behavior + 0.021*environ + 0.021*carbon + 0.019*auditorium + 0.016*map + 0.016*take + 0.015*abil + 0.014*great + 0.014*control + 0.012*jewett'),\n",
       " (24,\n",
       "  u'0.051*visual + 0.036*studi + 0.029*adolesc + 0.026*mind + 0.023*hous + 0.021*induc + 0.018*eat + 0.018*ideal + 0.017*relat + 0.016*concept'),\n",
       " (25,\n",
       "  u'0.046*s + 0.029*one + 0.023*south + 0.017*french + 0.017*mediev + 0.016*africa + 0.015*war + 0.015*civil + 0.014*china + 0.014*film'),\n",
       " (26,\n",
       "  u'0.038*love + 0.037*beyond + 0.022*artist + 0.019*method + 0.018*speech + 0.017*cycl + 0.016*therapi + 0.014*jazz + 0.014*k + 0.014*genr'),\n",
       " (27,\n",
       "  u'0.067*health + 0.054*cell + 0.045*effect + 0.037*care + 0.023*evid + 0.020*outcom + 0.019*growth + 0.018*food + 0.017*imagin + 0.016*cancer'),\n",
       " (28,\n",
       "  u'0.052*polit + 0.051*scienc + 0.034*center + 0.024*voic + 0.022*lead + 0.021*app + 0.016*natur + 0.016*panel + 0.015*inventor + 0.014*cognit'),\n",
       " (29,\n",
       "  u'0.044*power + 0.043*life + 0.026*histor + 0.021*s + 0.020*jewish + 0.016*inform + 0.016*red + 0.016*system + 0.015*special + 0.015*integr'),\n",
       " (30,\n",
       "  u'0.091*12 + 0.062*econom + 0.020*scienc + 0.019*histori + 0.017*refuge + 0.016*studi + 0.015*social + 0.014*park + 0.012*04 + 0.010*art'),\n",
       " (31,\n",
       "  u'0.036*differ + 0.030*factor + 0.026*time + 0.025*experi + 0.025*technolog + 0.018*go + 0.017*latina + 0.017*associ + 0.016*gestur + 0.016*role'),\n",
       " (32,\n",
       "  u'0.049*look + 0.037*book + 0.034*trade + 0.026*aid + 0.024*african + 0.023*vision + 0.019*climat + 0.018*art + 0.017*migrat + 0.015*wast'),\n",
       " (33,\n",
       "  u'0.027*children + 0.027*age + 0.027*effect + 0.025*violenc + 0.017*four + 0.016*school + 0.016*transform + 0.016*deaf + 0.013*scienc + 0.013*s'),\n",
       " (34,\n",
       "  u'0.057*perspect + 0.034*social + 0.023*gener + 0.022*statu + 0.020*issu + 0.019*video + 0.018*friendship + 0.016*interact + 0.015*optic + 0.015*protect'),\n",
       " (35,\n",
       "  u'0.047*13 + 0.037*examin + 0.030*law + 0.025*function + 0.021*ideolog + 0.020*song + 0.018*impact + 0.017*analysi + 0.016*big + 0.014*perceiv'),\n",
       " (36,\n",
       "  u'0.035*action + 0.033*exhibit + 0.029*hormon + 0.025*jewett + 0.024*understand + 0.019*galleri + 0.018*role + 0.018*student + 0.015*medicin + 0.015*sexta'),\n",
       " (37,\n",
       "  u'0.043*respons + 0.041*stress + 0.034*sp + 0.031*strain + 0.029*synechocysti + 0.028*pcc + 0.024*acid + 0.020*protein + 0.020*cyanobacterium + 0.020*determin'),\n",
       " (38,\n",
       "  u'0.054*two + 0.026*speak + 0.025*realiti + 0.018*republ + 0.018*predict + 0.014*advertis + 0.014*analysi + 0.014*cold + 0.014*theme + 0.013*look'),\n",
       " (39,\n",
       "  u'0.053*bodi + 0.029*imag + 0.023*chines + 0.020*self + 0.019*feminist + 0.018*femin + 0.017*domest + 0.015*marriag + 0.014*give + 0.013*assess'),\n",
       " (40,\n",
       "  u'0.041*india + 0.036*movement + 0.030*school + 0.023*peopl + 0.023*compar + 0.021*twentieth + 0.021*centuri + 0.020*film + 0.019*childhood + 0.016*explor'),\n",
       " (41,\n",
       "  u'0.052*project + 0.037*read + 0.035*freedom + 0.022*drug + 0.021*origin + 0.019*perspect + 0.019*parent + 0.017*daughter + 0.015*impact + 0.013*immigr'),\n",
       " (42,\n",
       "  u'0.051*evalu + 0.025*anti + 0.024*site + 0.024*religion + 0.022*group + 0.018*peptid + 0.018*effect + 0.012*portrait + 0.011*societi + 0.010*studi'),\n",
       " (43,\n",
       "  u'0.026*literatur + 0.023*good + 0.022*effect + 0.022*light + 0.021*mechan + 0.021*attent + 0.020*arabidopsi + 0.019*thaliana + 0.019*oligopeptidas + 0.019*thimet'),\n",
       " (44,\n",
       "  u'0.058*perform + 0.043*long + 0.042*music + 0.041*discuss + 0.033*art + 0.026*product + 0.021*panel + 0.017*target + 0.016*modern + 0.015*membran'),\n",
       " (45,\n",
       "  u'0.063*world + 0.043*chang + 0.038*make + 0.031*korean + 0.025*media + 0.021*war + 0.019*commun + 0.016*happi + 0.015*use + 0.015*social'),\n",
       " (46,\n",
       "  u'0.067*novel + 0.041*synthesi + 0.038*agent + 0.036*protein + 0.021*gold + 0.017*mice + 0.017*express + 0.015*mechan + 0.015*reform + 0.011*prepar'),\n",
       " (47,\n",
       "  u'0.083*ident + 0.044*space + 0.026*way + 0.025*new + 0.024*famili + 0.019*social + 0.018*capit + 0.018*white + 0.018*intervent + 0.018*place'),\n",
       " (48,\n",
       "  u'0.029*sociolog + 0.027*correl + 0.026*condit + 0.021*deriv + 0.020*neural + 0.019*identifi + 0.018*reduc + 0.018*philosophi + 0.016*ann + 0.014*current'),\n",
       " (49,\n",
       "  u'0.037*la + 0.027*direct + 0.025*moral + 0.022*justic + 0.019*secur + 0.018*de + 0.016*s + 0.014*healthi + 0.014*discours + 0.013*market')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=50, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
